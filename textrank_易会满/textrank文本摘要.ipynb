{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "前10个句子为：\n",
      "\n",
      "['信息量巨大', '易会满首秀，直面科创板8大问题，对散户加速入场笑而不语……', '每日经济新闻', '02-2717:56', '每经编辑：郭鑫 王晓波', '图片来源：新华社记者 李鑫 摄', '易会满上任一个月，还没有在公开场合说过一句话', '2月27日下午三点半开始，中国证监会主席易会满在北京国新办出席其首场新闻发布会，离发布会开始前两小时现场已经座无虚席，只等易主席来到现场', '此外，副主席李超、副主席方星海，上海证券交易所理事长黄红元等也共同出席', '这可能是这个月国内关注的人最多的一场新闻发布会了']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re,os,jieba\n",
    "from itertools import chain\n",
    "\n",
    "\"\"\"第一步：把文档划分成句子\"\"\"\n",
    "\n",
    "# 文档所在的文件夹\n",
    "c_root = os.getcwd()+os.sep+\"cnews\"+os.sep  \n",
    "\n",
    "sentences_list = []\n",
    "for file in os.listdir(c_root): \n",
    "    fp = open(c_root+file,'r',encoding=\"utf8\")\n",
    "    for line in fp.readlines():\n",
    "        if line.strip():\n",
    "            # 把元素按照[。！；？]进行分隔，得到句子。\n",
    "            line_split = re.split(r'[。！；？]',line.strip())\n",
    "            # [。！；？]这些符号也会划分出来，把它们去掉。\n",
    "            line_split = [line.strip() for line in line_split if line.strip() not in ['。','！','？','；'] and len(line.strip())>1]\n",
    "            sentences_list.append(line_split)\n",
    "sentences_list = list(chain.from_iterable(sentences_list))\n",
    "print(\"前10个句子为：\\n\")\n",
    "print(sentences_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一共有 347 个句子。\n",
      "\n",
      "前10个句子分词后的结果为：\n",
      " [['信息量'], ['易会', '满首秀', '直面', '科创板', '散户', '加速', '入场', '笑', '不语'], ['每日', '经济', '新闻'], [], ['每经', '编辑', '郭鑫', '王晓波'], ['图片', '来源', '李鑫', '摄'], ['易会', '上任', '一个月', '公开场合', '说', '一句', '话'], ['三点', '中国证监会', '主席', '易会', '北京', '国新办', '出席', '首场', '新闻', '发布会', '发布会', '前', '两', '小时', '现场', '座无虚席', '易', '主席', '来到', '现场'], ['副', '主席', '李超', '副', '主席', '星海', '上海证券交易所', '理事长', '黄红元', '出席'], ['国内', '关注', '一场', '新闻', '发布会']]\n",
      "\n",
      "数据预处理后句子的数量不变！\n"
     ]
    }
   ],
   "source": [
    "\"\"\"第二步：文本预处理，去除停用词和非汉字字符,并进行分词\"\"\"\n",
    "\n",
    "\n",
    "#创建停用词列表\n",
    "stopwords = [line.strip() for line in open('./stopwords.txt',encoding='UTF-8').readlines()]\n",
    "\n",
    "# 对句子进行分词\n",
    "def seg_depart(sentence):\n",
    "    # 去掉非汉字字符\n",
    "    sentence = re.sub(r'[^\\u4e00-\\u9fa5]+','',sentence)\n",
    "    sentence_depart = jieba.cut(sentence.strip())\n",
    "    word_list = []\n",
    "    for word in sentence_depart:\n",
    "        if word not in stopwords:\n",
    "            word_list.append(word)   \n",
    "    # 如果句子整个被过滤掉了，如：'02-2717:56'被过滤，那就返回[],保持句子的数量不变\n",
    "    return word_list\n",
    "\n",
    "sentence_word_list = []\n",
    "for sentence in sentences_list:   \n",
    "    line_seg = seg_depart(sentence)\n",
    "    sentence_word_list.append(line_seg)\n",
    "print(\"一共有\",len(sentences_list),'个句子。\\n')\n",
    "print(\"前10个句子分词后的结果为：\\n\",sentence_word_list[:10])\n",
    "\n",
    "# 保证处理后句子的数量不变，我们后面才好根据textrank值取出未处理之前的句子作为摘要。\n",
    "if len(sentences_list) == len(sentence_word_list):\n",
    "    print(\"\\n数据预处理后句子的数量不变！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一共有467140个词语/字。\n"
     ]
    }
   ],
   "source": [
    "\"\"\"第三步：准备词向量\"\"\"\n",
    "\n",
    "word_embeddings = {}\n",
    "f = open('./sgns.financial.char', encoding='utf-8')\n",
    "for line in f:\n",
    "    # 把第一行的内容去掉\n",
    "    if '467389 300\\n' not in line:\n",
    "        values = line.split()\n",
    "        # 第一个元素是词语\n",
    "        word = values[0]\n",
    "        embedding = np.asarray(values[1:], dtype='float32')\n",
    "        word_embeddings[word] = embedding\n",
    "f.close()\n",
    "print(\"一共有\"+str(len(word_embeddings))+\"个词语/字。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"第四步：得到词语的embedding，用WordAVG作为句子的向量表示\"\"\"\n",
    "\n",
    "sentence_vectors = []\n",
    "for i in sentence_word_list:\n",
    "    if len(i)!=0:\n",
    "        # 如果句子中的词语不在字典中，那就把embedding设为300维元素为0的向量。\n",
    "        # 得到句子中全部词的词向量后，求平均值，得到句子的向量表示\n",
    "        v = sum([word_embeddings.get(w, np.zeros((300,))) for w in i])/(len(i))\n",
    "    else:\n",
    "        # 如果句子为[]，那么就向量表示为300维元素为0个向量。\n",
    "        v = np.zeros((300,))\n",
    "    sentence_vectors.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "句子相似度矩阵的形状为： (347, 347)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"第四步：计算句子之间的余弦相似度，构成相似度矩阵\"\"\"\n",
    "sim_mat = np.zeros([len(sentences_list), len(sentences_list)])\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "for i in range(len(sentences_list)):\n",
    "  for j in range(len(sentences_list)):\n",
    "    if i != j:\n",
    "      sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,300), sentence_vectors[j].reshape(1,300))[0,0]\n",
    "print(\"句子相似度矩阵的形状为：\",sim_mat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1条摘要：\n",
      "\n",
      " 在新闻发布会上，易会满表示，我到证监会工作今天是31天，刚刚满月，是资本市场的新兵，从市场参与者到监管者，角色转换角色挑战很大，如履薄冰，不敢懈怠，唯恐辜负中央信任和市场期待，这也是我做好工作的动力，近期加强调查研究，和部门协作维护市场平稳发展，维护科创板前期基础工作 \n",
      "\n",
      "第2条摘要：\n",
      "\n",
      " 易会满在新闻发布会上表示，防止发生系统性风险是底线和根本任务，当前受国内外多种因素影响，资本市场风险形式严峻复杂，证监会将坚持精准施策，做好股票质押私募基金、场外配资和地方各类场所的重点领域风险的防范化解和处置工作，完善资本市场逆周期机制，健全及时反映风险波动系统，运用大数据、人工智能等手段对上市公司专业监管，平衡事前、事中、事后关系，监管端口前移，强化监管效能 \n",
      "\n",
      "第3条摘要：\n",
      "\n",
      " 证监会将坚持精准施策，做好股票质押私募基金、场外配资和地方各类场所的重点领域风险的防范化解和处置工作，完善资本市场逆周期机制，健全及时反映风险波动系统，运用大数据、人工智能等手段对上市公司专业监管，平衡事前、事中、事后关系，监管端口前移，强化监管效能，切实做好打铁必须自身硬，做好中介机构和高管的强监管 \n",
      "\n",
      "第4条摘要：\n",
      "\n",
      " 这两者出发点和规则不同，我来证监会后不断学习研究，这么专业的问题证监会有专业化的团队，资本市场是大的生态，什么叫市场，应该是依靠市场各参与者，调动市场参与者，市场规律办事， 培养健康生态比什么都重要，这一考验和要求比专业更重要，生态建设好了，资本市场的健康发展才有保证 \n",
      "\n",
      "第5条摘要：\n",
      "\n",
      " 证监会副主席李超今天也给市场吃下定心丸：“对二级市场影响的问题，（科创板）设立时已经高度关注，在一系列的制度、规则层面作了相应安排 \n",
      "\n",
      "第6条摘要：\n",
      "\n",
      " 他表示，第一，设立科创板主要目的是增强资本市场对实体经济的包容性，更好地服务具有核心技术、行业领先、有良好发展前景和口碑的企业，通过改革进一步完善支持创新的资本形成机制 \n",
      "\n",
      "第7条摘要：\n",
      "\n",
      " 一是提高宏观思维能力，贴近市场各参与方，坚持市场导向、法治导向、监管导向，加强对资本市场宏观战略问题的研究思考，加强顶层设计，增强战略定力，稳步推进重点关注问题的改革创新，在改革中、在发展中破解难题 \n",
      "\n",
      "第8条摘要：\n",
      "\n",
      " 集体学习的通稿中，中央给资本市场定的“法治化”要求有不少，比如“把好市场入口和市场出口两道关，加强全程监管”、“解决资本市场违法违规成本过低问题” \n",
      "\n",
      "第9条摘要：\n",
      "\n",
      " 易会满表示，证监会将以习近平新时代中国特色社会主义思想为指导，在国务院金融委的统一指挥协调下，主动加强与相关部委、地方党委政府和市场各方的沟通协作，努力形成工作合力，共同促进资本市场高质量发展 \n",
      "\n",
      "第10条摘要：\n",
      "\n",
      " 目前，资本市场已经回暖，这为改革提供了良好市场条件，我们要齐心协力，坚持“严标准、稳起步”的原则，积极做好落实和应对工作，注重各市场之间的平衡，确保改革平稳启动实施 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"第五步：迭代得到句子的textrank值，排序并取出摘要\"\"\"\n",
    "import networkx as nx\n",
    "\n",
    "# 利用句子相似度矩阵构建图结构，句子为节点，句子相似度为转移概率\n",
    "nx_graph = nx.from_numpy_array(sim_mat)\n",
    "\n",
    "# 得到所有句子的textrank值\n",
    "scores = nx.pagerank(nx_graph)\n",
    "\n",
    "# 根据textrank值对未处理的句子进行排序\n",
    "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences_list)), reverse=True)\n",
    "\n",
    "# 取出得分最高的前10个句子作为摘要\n",
    "sn = 10\n",
    "for i in range(sn):\n",
    "    print(\"第\"+str(i+1)+\"条摘要：\\n\\n\",ranked_sentences[i][1],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
